{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d2d372b",
   "metadata": {},
   "source": [
    "1. Imports and Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac72fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "PAGE_IMAGES_DIR = DATA_DIR / \"page_images\"\n",
    "PAGE_IMAGES_DIR.mkdir(exist_ok=True)\n",
    "FAISS_INDEX_PATH = DATA_DIR / \"faiss_index.bin\"\n",
    "META_PATH = DATA_DIR / \"meta.json\"\n",
    "\n",
    "# Configuration\n",
    "IMAGE_DPI = 300\n",
    "TOP_K_PAGES = 5\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3bc41e",
   "metadata": {},
   "source": [
    "2. Convert PDF → page images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8df7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "\n",
    "def pdf_to_page_images(pdf_path: str, out_dir: Path, dpi: int = IMAGE_DPI) -> List[Path]:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    images = convert_from_path(pdf_path, dpi=dpi)\n",
    "    saved = []\n",
    "    for i, img in enumerate(images, start=1):\n",
    "        p = out_dir / f\"{Path(pdf_path).stem}_page_{i:04d}.png\"\n",
    "        img.save(p, format=\"PNG\")\n",
    "        saved.append(p)\n",
    "    return saved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae05999b",
   "metadata": {},
   "source": [
    "3. Embed page images with Jina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ee3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "JINA_API_KEY = os.getenv(\"JINA_API_KEY\")\n",
    "JINA_EMBED_URL = \"https://api.jina.ai/v1/embed\"\n",
    "\n",
    "def embed_images_with_jina(image_paths: List[Path], model: str = \"jina-embeddings-v3\") -> np.ndarray:\n",
    "    if not JINA_API_KEY:\n",
    "        raise RuntimeError(\"No JINA_API_KEY in env\")\n",
    "    headers = {\"Authorization\": f\"Bearer {JINA_API_KEY}\"}\n",
    "    embeddings = []\n",
    "    for p in image_paths:\n",
    "        with open(p, \"rb\") as f:\n",
    "            b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "        payload = {\"model\": model, \"mode\": \"image\", \"data\": [b64]}\n",
    "        resp = requests.post(JINA_EMBED_URL, json=payload, headers=headers, timeout=60)\n",
    "        resp.raise_for_status()\n",
    "        emb = resp.json()[\"data\"][0][\"embedding\"]\n",
    "        embeddings.append(np.array(emb, dtype=np.float32))\n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2397c8b4",
   "metadata": {},
   "source": [
    "4. Store embeddings in FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4218442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss, json\n",
    "\n",
    "def build_faiss_index(embeddings, meta, index_path=\"faiss.idx\", meta_path=\"meta.json\"):\n",
    "    d = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, index_path)\n",
    "    with open(meta_path, \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bd5412",
   "metadata": {},
   "source": [
    "5. Query → embed with Jina → retrieve relevant pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa36ceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jina_text_embed(query, model=\"jina-embeddings-v3\"):\n",
    "    headers = {\"Authorization\": f\"Bearer {JINA_API_KEY}\"}\n",
    "    payload = {\"model\": model, \"input\": [query]}\n",
    "    r = requests.post(JINA_EMBED_URL, json=payload, headers=headers)\n",
    "    r.raise_for_status()\n",
    "    return np.array(r.json()[\"data\"][0][\"embedding\"], dtype=np.float32).reshape(1, -1)\n",
    "\n",
    "def retrieve(query, k=5, index_path=\"faiss.idx\", meta_path=\"meta.json\"):\n",
    "    q_emb = jina_text_embed(query)\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    index = faiss.read_index(index_path)\n",
    "    D, I = index.search(q_emb, k)\n",
    "    with open(meta_path) as f:\n",
    "        meta = json.load(f)\n",
    "    results = [{\"score\": float(s), \"meta\": meta[i]} for s, i in zip(D[0], I[0])]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466f7ff9",
   "metadata": {},
   "source": [
    "6. Parse retrieved pages with unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773233a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.auto import partition\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "def parse_page(image_path):\n",
    "    result = {\"path\": image_path, \"text\": \"\"}\n",
    "    try:\n",
    "        elements = partition(filename=image_path)\n",
    "        texts = [e.text for e in elements if hasattr(e, \"text\") and e.text]\n",
    "        result[\"text\"] = \"\\n\".join(texts)\n",
    "    except:\n",
    "        result[\"text\"] = pytesseract.image_to_string(Image.open(image_path))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cf08e8",
   "metadata": {},
   "source": [
    "7. RAPTOR-like index & answer query (local LLMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434065a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from transformers import pipeline\n",
    "\n",
    "text_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  \n",
    "summarizer = pipeline(\"summarization\", model=\"google/flan-t5-small\")\n",
    "\n",
    "def chunk_text(text, size=400, overlap=50):\n",
    "    words, chunks = text.split(), []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        chunks.append(\" \".join(words[i:i+size]))\n",
    "        i += size - overlap\n",
    "    return chunks\n",
    "\n",
    "def build_clusters(pages):\n",
    "    chunks, metas = [], []\n",
    "    for p in pages:\n",
    "        for c in chunk_text(p[\"text\"]):\n",
    "            chunks.append(c)\n",
    "            metas.append(p[\"path\"])\n",
    "    if not chunks: return []\n",
    "    embs = text_model.encode(chunks, normalize_embeddings=True)\n",
    "    n_clusters = max(2, int(len(chunks) ** 0.5))\n",
    "    labels = AgglomerativeClustering(n_clusters=n_clusters).fit_predict(embs)\n",
    "    clusters = {}\n",
    "    for lab, chunk in zip(labels, chunks):\n",
    "        clusters.setdefault(lab, []).append(chunk)\n",
    "    cluster_summaries = []\n",
    "    for lab, chs in clusters.items():\n",
    "        text = \" \".join(chs)[:1000]\n",
    "        summary = summarizer(text, max_length=100, min_length=20)[0][\"summary_text\"]\n",
    "        cluster_summaries.append(summary)\n",
    "    return cluster_summaries\n",
    "\n",
    "def answer_query(query, cluster_summaries):\n",
    "    context = \"\\n\\n\".join(cluster_summaries[:3])\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    ans = summarizer(prompt, max_length=150, min_length=30)[0][\"summary_text\"]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffba1fbc",
   "metadata": {},
   "source": [
    "Usage Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bbf953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert PDF to images\n",
    "pages = pdf_to_page_images(\"3M_2017_10K.pdf\", \"pages/\")\n",
    "\n",
    "# 2. Embed with Jina\n",
    "embs = embed_images_with_jina(pages)\n",
    "\n",
    "# 3. Index in FAISS\n",
    "build_faiss_index(embs, [{\"path\": p} for p in pages])\n",
    "\n",
    "# 4. Retrieve\n",
    "hits = retrieve(\"Q4 revenue growth\")\n",
    "print(\"Top hits:\", hits)\n",
    "\n",
    "# 5. Parse retrieved\n",
    "parsed = [parse_page(h[\"meta\"][\"path\"]) for h in hits]\n",
    "\n",
    "# 6. Build RAPTOR index + answer\n",
    "clusters = build_clusters(parsed)\n",
    "print(answer_query(\"What was the Q4 revenue growth?\", clusters))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
